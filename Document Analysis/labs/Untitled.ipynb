{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237561bd-7623-4d14-b202-1057bbd79a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bisect import bisect_right\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d302a-5015-4ee0-893d-d5b4c2ccec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "DEVICE = torch.device('cuda')  # change this if you want to use GPU\n",
    "PAD_IDX = 0\n",
    "CLS_TOKEN = '[CLS]'  # the special [CLS] token to be prepended to each sequence\n",
    "SEED = 101\n",
    "\n",
    "tokeniser = nltk.tokenize.TreebankWordTokenizer()\n",
    "stopwords = frozenset(nltk.corpus.stopwords.words(\"english\"))\n",
    "trans_table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "def tokenise_text(str_):\n",
    "    \"\"\"Tokenize a string of text.\n",
    "\n",
    "    Args:\n",
    "        str_: The input string of text.\n",
    "\n",
    "    Returns:\n",
    "        list(str): A list of tokens.\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "def prepare_dataset(filename):\n",
    "    \"\"\"Prepare the training/validation/test dataset.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of file from which data will be loaded.\n",
    "\n",
    "    Returns:\n",
    "        Xr_train (iterable(str)): Documents in the training set, each\n",
    "            represented as a string.\n",
    "        y_train (np.ndarray): A vector of class labels for documents in\n",
    "            the training set, each element of the vector is either 0 or 1.\n",
    "        Xr_val (iterable(str)): Documents in the validation set, each\n",
    "            represented as a string.\n",
    "        y_val (np.ndarray): A vector of class labels for documents in\n",
    "            the validation set, each element of the vector is either 0 or 1.\n",
    "        Xr_test (iterable(str)): Documents in the test set, each\n",
    "            represented as a string.\n",
    "        y_test (np.ndarray): A vector of class labels for documents in\n",
    "            the test set, each element of the vector is either 0 or 1.\n",
    "    \"\"\"\n",
    "    print('Preparing train/val/test dataset ...')\n",
    "    # load raw data\n",
    "    df = pd.read_csv(filename)\n",
    "    # shuffle the rows\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    # get the train, val, test splits\n",
    "    train_frac, val_frac, test_frac = 0.7, 0.1, 0.2\n",
    "    Xr = df[\"text\"].tolist()\n",
    "    train_end = int(train_frac*len(Xr))\n",
    "    val_end = int((train_frac + val_frac)*len(Xr))\n",
    "    Xr_train = Xr[0:train_end]\n",
    "    Xr_val = Xr[train_end:val_end]\n",
    "    Xr_test = Xr[val_end:]\n",
    "\n",
    "    # encode sentiment labels ('pos' and 'neg')\n",
    "    yr = df[\"label\"].tolist()\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(yr)\n",
    "    y_train = np.array(y[0:train_end])\n",
    "    y_val = np.array(y[train_end:val_end])\n",
    "    y_test = np.array(y[val_end:])\n",
    "    return Xr_train, y_train, Xr_val, y_val, Xr_test, y_test\n",
    "\n",
    "\n",
    "def build_vocab(Xt, min_freq=1):\n",
    "    \"\"\"Create a list of sentences, build the vocabulary and compute word frequencies from the given text data.\n",
    "\n",
    "    Args:\n",
    "        Xr (iterable(str)): A list of strings each representing a document.\n",
    "        min_freq: The minimum frequency of a token that will be kept in the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        vocab (dict(str : int)): A dictionary mapping a word/token to its index.\n",
    "    \"\"\"\n",
    "    print('Building vocabulary ...')\n",
    "    counter = Counter()\n",
    "    for xt in Xt:\n",
    "        counter.update(xt)\n",
    "    sorted_token_freq_pairs = counter.most_common()\n",
    "\n",
    "    # find the first index where freq=min_freq-1 in sorted_token_freq_pairs using binary search/bisection\n",
    "    end = bisect_right(sorted_token_freq_pairs, -min_freq, key=lambda x: -x[1])\n",
    "    vocab = {token: idx+PAD_IDX+1 for (idx, (token, freq)) in enumerate(sorted_token_freq_pairs[:end])}  # PAD_IDX is reserved for padding\n",
    "    vocab[CLS_TOKEN] = len(vocab) + PAD_IDX\n",
    "\n",
    "    print(f'Vocabulary size: {len(vocab)}')\n",
    "    return vocab\n",
    "\n",
    "\n",
    "class MovieReviewDataset(Dataset):\n",
    "    \"\"\"A Dataset to be used by a data loader.\n",
    "    See https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, X_all, y_all, cls_idx, max_seq_len):\n",
    "        # X_all, y_all are the labelled examples\n",
    "        # cls_idx is the index of token '[CLS]' in the vocabulary\n",
    "        # max_seq_len is the maximum length of a sequence allowed\n",
    "        self.X_all = X_all\n",
    "        self.y_all = y_all\n",
    "        self.cls_idx = cls_idx\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_all)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # prepend the index of the special token '[CLS]' to each sequence\n",
    "        x = [self.cls_idx] + self.X_all[idx]\n",
    "        # truncate a sequence if it is longer than the maximum length allowed\n",
    "        if len(x) > self.max_seq_len:\n",
    "            x = x[:self.max_seq_len]\n",
    "        return x, self.y_all[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Merges a list of samples to form a mini-batch for model training/evaluation.\n",
    "    To be used by a data loader.\n",
    "    See https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "    \"\"\"\n",
    "    Xb = pad_sequence([torch.tensor(x, dtype=torch.long) for (x, _) in batch], padding_value=PAD_IDX)\n",
    "    yb = torch.tensor([y for (_, y) in batch], dtype=torch.float32)\n",
    "    return Xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "\n",
    "def get_positional_encoding(emb_size, max_seq_len):\n",
    "    \"\"\"Compute the positional encoding.\n",
    "\n",
    "    Args:\n",
    "        emb_size (int): the dimension of positional encoding\n",
    "        max_seq_len (int): the maximum allowed length of a sequence\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: positional encoding, size=(max_seq_len, emb_size)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: compute the positional encoding as specified in the question.\n",
    "    PE = torch.zeros(max_seq_len, emb_size)\n",
    "    for t in range(max_seq_len):\n",
    "        for i in range(emb_size // 2):\n",
    "            PE[t][2 * i] = math.sin(t / 10000 ** (2 * i / emb_size))\n",
    "            PE[t][2 * i + 1] = math.cos(t / 10000 ** (2 * i / emb_size))\n",
    "    return PE\n",
    "\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    \"\"\"A movie review sentiment classifier using transformers.\"\"\"\n",
    "    def __init__(self, vocab_size, emb_size=128, ffn_size=128, num_tfm_layer=2, num_head=2, p_dropout=0.2, max_seq_len=300):\n",
    "        \"\"\"SentimentClassifier initialiser.\n",
    "        Args:\n",
    "            vocab_size (int): the size of vocabulary\n",
    "            emb_size (int): the dimension of token embedding (and position encoding)\n",
    "            ffn_size (int): the dimension of the feedforward network model in a transformer encoder layer\n",
    "            num_tfm_layer (int): the number of transformer encoder layers\n",
    "            p_dropout (float): the dropout probability (to be used in a transformer encoder layer as well as the dropout\n",
    "                layer of this class.\n",
    "            max_seq_len (int): the maximum allowed length of a sequence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "\n",
    "        # registers the positional encoding so that it is saved with the model\n",
    "        # see https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\n",
    "        self.register_buffer(\n",
    "            \"positional_encoding\", get_positional_encoding(emb_size, max_seq_len), persistent=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=p_dropout)\n",
    "        self.linear = nn.Linear(emb_size, 1)  # for binary classification\n",
    "\n",
    "        # TODO: create a TransformerEncoder with `num_tfm_layer` TransformerEncoderLayer, see\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n",
    "        self.transformerLayer = nn.TransformerEncoderLayer(emb_size, num_head, ffn_size, p_dropout)\n",
    "        self.transformer = nn.TransformerEncoder(self.transformerLayer, num_tfm_layer)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function of SentimentClassifier.\n",
    "        x: a (mini-batch) of samples, size=(SEQUENCE_LENGTH, BATCH_SIZE)\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: implement the forward function as specified in the question\n",
    "        mask = (x == PAD_IDX).t()\n",
    "        x = self.token_embeddings(x)\n",
    "        #print(x.size())\n",
    "        x += self.positional_encoding[:x.size(0), :].unsqueeze(1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        output = self.transformer.forward(x, src_key_padding_mask=mask)\n",
    "        final_hidden_state = output[-1]\n",
    "        logits = self.linear(final_hidden_state)\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "def eval_model(model, dataset, batch_size=64):\n",
    "    \"\"\"Evaluate a trained SentimentClassifier.\n",
    "\n",
    "    Args:\n",
    "        model (SentimentClassifier): a trained model\n",
    "        dataset (MovieReviewDataset): a dataset of samples\n",
    "        batch_size (int): the batch_size\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the provided dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        targets = []\n",
    "        for (Xb, yb) in tqdm(dataloader):\n",
    "            out = model(Xb)\n",
    "            preds.append(out.cpu().numpy() > 0)\n",
    "            targets.append(yb.cpu().numpy())\n",
    "\n",
    "        score = accuracy_score(np.concatenate(targets), np.concatenate(preds).astype(np.int32))\n",
    "\n",
    "    model.train()\n",
    "    return score\n",
    "\n",
    "\n",
    "def train_model(model, dataset_train, dataset_val, batch_size=64, num_epoch=1, learning_rate=0.0001, fmodel='best_model.pth'):\n",
    "    \"\"\"Train a SentimentClassifier.\n",
    "\n",
    "    Args:\n",
    "        model (SentimentClassifier): a model to be trained\n",
    "        dataset_train (MovieReviewDataset): a dataset of samples (training set)\n",
    "        dataset_val (MovieReviewDataset): a dataset of samples (validation set)\n",
    "        batch_size (int): the batch_size\n",
    "        num_epoch (int): the number of training epochs\n",
    "        learning_rate (float): the learning rate\n",
    "        fmodel (str): name of file to save the model that achieves the best accuracy on the validation set\n",
    "\n",
    "    Returns:\n",
    "        SentimentClassifier: the trained model\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()  # the binary cross entropy loss using logits\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    num_batch = (len(X_train) - 1) // batch_size + 1\n",
    "    print(f'{\"Epoch\":>10} {\"Batch\":>15} {\"Train loss (running avg.)\":>25}')\n",
    "\n",
    "    # TODO: train the model for `num_epoch` epochs using the training set\n",
    "    # evaluate the model on the validation set after each epoch of training\n",
    "    # save the model that achieves the best accuracy on the validation set\n",
    "    # see https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "    x_train_loader = DataLoader(dataset_train, batch_size = batch_size, shuffle = True, collate_fn=collate_fn)\n",
    "    #x_val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        loss = 0\n",
    "        #preds=[]\n",
    "        best_score = 0\n",
    "        for (Xb, yb) in tqdm(x_train_loader):\n",
    "            optimiser.zero_grad()\n",
    "            out = model(Xb)\n",
    "            #preds.append(out.cpu.numpy() > 0)\n",
    "            batch_loss = loss_fn(out, yb)\n",
    "            batch_loss.backward()\n",
    "            optimiser.step()\n",
    "            loss += batch_loss.item()\n",
    "            #print(f'{epoch + 1:>10} {i:>15} {loss / i:>25.5f}', end='\\r')\n",
    "        score = eval_model(model,dataset_val,num_batch)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            torch.save(model.state_dict(),fmodel)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    fmodel = 'best_model.pth'\n",
    "    data_file = os.path.join(\"data\", \"movie_reviews_labelled.csv\")\n",
    "    Xr_train, y_train, Xr_val, y_val, Xr_test, y_test = prepare_dataset(filename=data_file)\n",
    "\n",
    "    get_tokenised_docs = lambda Xr: [tokenise_text(xr) for xr in tqdm(Xr)]\n",
    "    get_token_indices = lambda Xt, vocab: [[vocab[token] for token in xt if token in vocab] for xt in Xt]\n",
    "\n",
    "    Xt_train, Xt_val, Xt_test = [get_tokenised_docs(Xr) for Xr in [Xr_train, Xr_val, Xr_test]]\n",
    "    vocab = build_vocab(Xt_train + Xt_val, min_freq=5)  # you may use a different min_freq\n",
    "    X_train, X_val, X_test = [get_token_indices(Xt, vocab) for Xt in [Xt_train, Xt_val, Xt_test]]\n",
    "\n",
    "    max_seq_len = 500  # you may use a different max_seq_len\n",
    "    cls_idx = vocab[CLS_TOKEN]\n",
    "    dataset_train = MovieReviewDataset(X_train, y_train, cls_idx, max_seq_len)\n",
    "    dataset_val = MovieReviewDataset(X_val, y_val, cls_idx, max_seq_len)\n",
    "    dataset_test = MovieReviewDataset(X_test, y_test, cls_idx, max_seq_len)\n",
    "\n",
    "    # Note that we do not directly use the combined training set and validation set to re-train the model\n",
    "    # we use this strategy for simplicity,\n",
    "    # see Section 7.8 in the deep learning textbook for other possible options\n",
    "    # at https://www.deeplearningbook.org/contents/regularization.html\n",
    "\n",
    "    # you may use different values for these hyper-parameters\n",
    "    clf = SentimentClassifier(\n",
    "        len(vocab),\n",
    "        emb_size = 300,\n",
    "        ffn_size = 512,\n",
    "        num_tfm_layer = 2,\n",
    "        num_head = 4,\n",
    "        p_dropout = 0.5,\n",
    "        max_seq_len = max_seq_len,\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # you may use different values for the hyper-parameters\n",
    "    clf = train_model(clf, dataset_train, dataset_val, batch_size=64, num_epoch=50, learning_rate=3e-4, fmodel=fmodel)\n",
    "\n",
    "    print(f'Loading model from {fmodel} ...')\n",
    "    clf.load_state_dict(torch.load(fmodel, map_location=torch.device(DEVICE)))\n",
    "    clf = clf.to(DEVICE)\n",
    "    print(clf)\n",
    "\n",
    "    acc_test = eval_model(clf, dataset_test)\n",
    "    print(f'Accuracy (test): {acc_test:.4f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
