{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efadd59-b8c3-4503-96f9-551aa8ae04c8",
   "metadata": {},
   "source": [
    "# Fine-tuning a Pre-trained Transformer for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7b30b-6bb4-4140-a43d-d47b4e6c9ddf",
   "metadata": {},
   "source": [
    "In this lab you will fine-tune a pre-trained transformer model using the huggingface `transformers` library. This library provides a number of transformer models such as BERT, XLNet, and GPT, that can be used with PyTorch or Tensorflow. The tokenisers for these models are also included, which makes using transformers with this library much easier than developing them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c79194-7436-4f62-b871-d05c70bc8b92",
   "metadata": {},
   "source": [
    "The [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) which is a smaller version of BERT will be used in this lab. It is much faster to fine-tune, but will give slightly worse performance than the original BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e48ea-0371-4f75-a0c7-ce8558d5f63f",
   "metadata": {},
   "source": [
    "For the dataset we will use the [IMDB movie review data](https://huggingface.co/datasets/imdb) where the task is to classify a review as either positive if the reviewer liked the movie or negative if the reviewer did not like the movie. The input is the text of the review and the output is a binary label either 0 (negative) or 1 (positive).\n",
    "In previous labs and the assignments, you have explored this dataset multiple times but with a differnt train/validation/test splits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d17cb-dfcb-4934-aa04-5d00e30e59a0",
   "metadata": {},
   "source": [
    "We will use both `pytorch` and the `transformers` library, as well as a few other useful libraries such as `tqdm` to make progress bars, and `sklearn` for its evaluation metric.\n",
    "\n",
    "We will also use the `datasets` library which is the huggingface datasets library and can be installed using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264b8ca-f0cf-4fe0-b356-ca24091bda87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6fa4ff-a38d-409f-8845-1b26f6e93de4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ab68a8-0652-4cc5-96aa-b946404fdc76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02357842-704f-4be0-a32f-e59e8187a295",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "First, load the `imdb` data using the huggingface `datasets` library, which will automatically download the data if it is not already cached on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7b8631-e6fc-4ca6-932a-c6fd6adbd3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a199607713db4b16bc89aecd4e35a9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13dc3915b514439b3c40e826ccd3b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273bf33f561c4a5d9e3c011ccdee9d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text to /Users/juneehome/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5c194780f044818c4590902e67aa74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /Users/juneehome/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fe6f4e29e445028fa9dc11344a55aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5642686-66a4-4372-83d6-880f9fefcd0f",
   "metadata": {},
   "source": [
    "If you print the `data` variable you will see that you have a dictionary like object with keys `test`, `train`, `unsupervised`, and values of `TorchIterableDataset`. We will only use the `test` and `train` data for this lab. You can inspect the first element of the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88193e5-a0cc-43bc-a186-ff7b1875aede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the '\n",
      "         'controversy that surrounded it when it was first released in 1967. I '\n",
      "         'also heard that at first it was seized by U.S. customs if it ever '\n",
      "         'tried to enter this country, therefore being a fan of films '\n",
      "         'considered \"controversial\" I really had to see this for myself.<br '\n",
      "         '/><br />The plot is centered around a young Swedish drama student '\n",
      "         'named Lena who wants to learn everything she can about life. In '\n",
      "         'particular she wants to focus her attentions to making some sort of '\n",
      "         'documentary on what the average Swede thought about certain '\n",
      "         'political issues such as the Vietnam War and race issues in the '\n",
      "         'United States. In between asking politicians and ordinary denizens '\n",
      "         'of Stockholm about their opinions on politics, she has sex with her '\n",
      "         'drama teacher, classmates, and married men.<br /><br />What kills me '\n",
      "         'about I AM CURIOUS-YELLOW is that 40 years ago, this was considered '\n",
      "         'pornographic. Really, the sex and nudity scenes are few and far '\n",
      "         \"between, even then it's not shot like some cheaply made porno. While \"\n",
      "         'my countrymen mind find it shocking, in reality sex and nudity are a '\n",
      "         'major staple in Swedish cinema. Even Ingmar Bergman, arguably their '\n",
      "         'answer to good old boy John Ford, had sex scenes in his films.<br '\n",
      "         '/><br />I do commend the filmmakers for the fact that any sex shown '\n",
      "         'in the film is shown for artistic purposes rather than just to shock '\n",
      "         'people and make money to be shown in pornographic theaters in '\n",
      "         'America. I AM CURIOUS-YELLOW is a good film for anyone wanting to '\n",
      "         'study the meat and potatoes (no pun intended) of Swedish cinema. But '\n",
      "         \"really, this film doesn't have much of a plot.\"}\n"
     ]
    }
   ],
   "source": [
    "pprint(data['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4bb87-c4db-4707-b07f-06713cbf9d97",
   "metadata": {},
   "source": [
    "If you do this you will get a dictionary with both the `label` and the `text` of the first element. The label indicates if this text is a positive or negative review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8a0ca8-bd5d-4fbe-af17-642dd8580666",
   "metadata": {},
   "source": [
    "### Tokeniser\n",
    "\n",
    "Since the `text` is currently a string of characters we need to:\n",
    "1. Split the sequence into tokens (i.e. word-pieces for BERT models but will be different depending on the pre-trained model)\n",
    "2. Represent the sequence as token/word-piece ids\n",
    "3. Add a `[CLS]` token to the start of the sequence, and a `[SEP]` token to the end\n",
    "4. Pad the sequences with `0`'s so they are all the same length\n",
    "5. Construct an attention mask for the input (to identify which parts of the input are padding and so should be ignored)\n",
    "\n",
    "Fortunately, the `transformers` library provides an easy way to do all of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e6d3e-00fe-4e07-88ed-0dafcfaf3c46",
   "metadata": {},
   "source": [
    "First, get the tokeniser specific to DistilBert ([documentation here](https://huggingface.co/docs/transformers/model_doc/distilbert)). \\\n",
    "This will download the tokeniser for the lowercase only version of DistilBert. The download is only a few kilobytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6751491b-b420-48fd-b44e-080f59ef25a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be2a485878d4ff1a0ca49738a71ae16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4859c9b2188c4c619ecc1f3fe6236daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc077b8196d465ba5d8860ff6246982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bd62fba45d41178f4049b5af531efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tok = DistilBertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0e1df-14a7-464e-b766-7d1455feaad6",
   "metadata": {},
   "source": [
    "Next, write a function to apply the tokeniser to the `text` field in the dataset.\n",
    "\n",
    "This will do all the steps 1-5 listed above and return the attention mask and the sequence of ids. The arguments indicate we are truncating sequences longer than the maximum length, and padding all sequences that are less than the maximum length so that they are exactly the maximum length. The maximum length for the DistilBert model is 512 word-pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e46fed0-54e8-4997-90fd-aa9d22a0a984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(X):\n",
    "    return tok(X[\"text\"], truncation = True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f559facb-191c-4767-8daf-3726021f1512",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72026794-0265-4e92-a549-856242cff977",
   "metadata": {},
   "source": [
    "Now we can apply the tokeniser to the text as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80bd979-9f01-402c-a755-27718a6b1cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shuffle_train_data = data['train'].shuffle(seed=42)\n",
    "tokenized_train_data = shuffle_train_data.map(tokenize_fn, batched=True)\n",
    "small_train_data = tokenized_train_data.select(range(1000))\n",
    "small_train_data = small_train_data.with_format(\"torch\", columns=[\"label\", \"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a9645-a2bf-4c65-b41a-93a1a541f762",
   "metadata": {},
   "source": [
    "The part which actually applies the tokeniser is the `map` call. This will call the `tokenize_fn` function on each of the examples in `data['train']`.\n",
    "\n",
    "The `shuffle` ensures the data is in a random order (if you don't do this you will run into problems because the dataset has all the 0 classes first followed by all the 1 classes).\n",
    "\n",
    "The `select` statement just extracts the first 1000 examples (of the shuffled list), if you were training this model to get the best performance you would use all the examples but for this lab we will use only 1000 to avoid waiting around for the model to train.\n",
    "\n",
    "The final line which calls `with_format` is responsible for converting the data columns `label`, `input_ids`, and `attention_mask` into PyTorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753957f-05eb-41b9-b7d4-f78cc55bfb0f",
   "metadata": {},
   "source": [
    "It is worthwhile to inspect `small_train_data` at this point. To do this use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2682d99d-3a1a-487e-a258-fef033e4595e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]),\n",
      " 'input_ids': tensor([  101,  2045,  2003,  2053,  7189,  2012,  2035,  2090,  3481,  3771,\n",
      "         1998,  6337,  2099,  2021,  1996,  2755,  2008,  2119,  2024,  2610,\n",
      "         2186,  2055,  6355,  6997,  1012,  6337,  2099,  3504, 15594,  2100,\n",
      "         1010,  3481,  3771,  3504,  4438,  1012,  6337,  2099, 14811,  2024,\n",
      "         3243,  3722,  1012,  3481,  3771,  1005,  1055,  5436,  2024,  2521,\n",
      "         2062,  8552,  1012,  1012,  1012,  3481,  3771,  3504,  2062,  2066,\n",
      "         3539,  8343,  1010,  2065,  2057,  2031,  2000,  3962, 12319,  1012,\n",
      "         1012,  1012,  1996,  2364,  2839,  2003,  5410,  1998,  6881,  2080,\n",
      "         1010,  2021,  2031,  1000, 17936,  6767,  7054,  3401,  1000,  1012,\n",
      "         2111,  2066,  2000, 12826,  1010,  2000,  3648,  1010,  2000, 16157,\n",
      "         1012,  2129,  2055,  2074,  9107,  1029,  6057,  2518,  2205,  1010,\n",
      "         2111,  3015,  3481,  3771,  3504,  2137,  2021,  1010,  2006,  1996,\n",
      "         2060,  2192,  1010,  9177,  2027,  9544,  2137,  2186,  1006,   999,\n",
      "          999,   999,  1007,  1012,  2672,  2009,  1005,  1055,  1996,  2653,\n",
      "         1010,  2030,  1996,  4382,  1010,  2021,  1045,  2228,  2023,  2186,\n",
      "         2003,  2062,  2394,  2084,  2137,  1012,  2011,  1996,  2126,  1010,\n",
      "         1996,  5889,  2024,  2428,  2204,  1998,  6057,  1012,  1996,  3772,\n",
      "         2003,  2025, 23105,  2012,  2035,  1012,  1012,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]),\n",
      " 'label': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "pprint(small_train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b2636-af9c-40ec-98a3-22c60075da3d",
   "metadata": {},
   "source": [
    "You will see that the datapoint has an `attention_mask`, `input_ids`, and `label`. The `attention_mask` and `input_ids` are new and were added by `tokenize_fn`.\n",
    "\n",
    "The `input_ids` are the padded sequences of word-piece ids, while the `attention_mask` identifies which parts of the `input_ids` are padding and so should be ignored by the attention layer. \n",
    "\n",
    "Note that the first id in the tensor of `input_ids` is `101`, which represents the `[CLS]` token, while the last non-zero id is `102` which is the `[SEP]` token. If you want to explore a bit further the mapping from tokens to ids can be accessed through the dictionary `tok.vocab`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898cd26-cfbe-4b42-b41f-34e434df7e3e",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76904945-5d93-4640-9798-ce38caa091c4",
   "metadata": {},
   "source": [
    "We can now create a validation dataset from the `test` data in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccc1d964-01f7-4bb5-9615-ec5ecfa6a051",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shuffle_val_data = data['test'].shuffle(seed=42)\n",
    "tokenized_val_data = shuffle_val_data.map(tokenize_fn, batched=True)\n",
    "small_val_data = tokenized_val_data.select(range(1000))\n",
    "small_val_data = small_val_data.with_format(\"torch\", columns=[\"label\", \"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0e4ac-3015-45e5-81e0-f74c8ce4e932",
   "metadata": {},
   "source": [
    "## Model\n",
    "Defining and downloading the pre-trained transformer is straightforward but be aware that the download is ~270M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490bc3f8-952c-4ba5-a86f-8eafae50bfe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5522218ba2314711bc75233d129ced55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m DistilBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:2432\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2418\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   2419\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   2421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   2431\u001b[0m     }\n\u001b[0;32m-> 2432\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs)\n\u001b[1;32m   2434\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   2435\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   2436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   2437\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    414\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[1;32m    420\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    421\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    422\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    423\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    424\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    425\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    426\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    427\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    428\u001b[0m         use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_file_manager() \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[1;32m   1362\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, temp_file\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m-> 1364\u001b[0m     http_get(\n\u001b[1;32m   1365\u001b[0m         url_to_download,\n\u001b[1;32m   1366\u001b[0m         temp_file,\n\u001b[1;32m   1367\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1368\u001b[0m         resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[1;32m   1369\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1370\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    531\u001b[0m     displayed_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(…)\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_name[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m progress \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[1;32m    534\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m     unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(logger\u001b[38;5;241m.\u001b[39mgetEffectiveLevel() \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mNOTSET),\n\u001b[1;32m    540\u001b[0m )\n\u001b[0;32m--> 541\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m         progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9d737-b4c1-43d4-a958-36f1d90e8773",
   "metadata": {},
   "source": [
    "The above specifies downloads and sets-up a pre-trained `DistilBert` model and configures it for the sequence classification task. \n",
    "\n",
    "The download is specifically the [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) pre-trained model, which only considers lowercase characters (there is a cased version available also `distilbert-base-cased`). \n",
    "\n",
    "Under the hood, the huggingface sequence classifier model will feed the output of the transformer (the output value at the position of the `[CLS]` token) to a new Linear layer to get logits (un-normalised scores for each of the classes). We need to provide the number of classes so the linear layers weight matrix can be properly specified. The way to do this is with the `num_labels` argument. As there are only two classes in our task (positive and negative) we specify `2` as the number of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390874e5-18de-446b-8f94-2ca7b17aefab",
   "metadata": {},
   "source": [
    "The loaded `DistilBert` model is in the variable `model` which is also a PyTorch module with the normal set of PyTorch methods, such as `forward`, `parameters`, and `to`.\n",
    "\n",
    "We first move the model to the desired device such as the GPU (if available) using `model.to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb6c6f-861f-46c5-a411-99c1c7857e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25aa483-7ac4-4b80-abc9-5705a8d39f9c",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da3ff6-443c-4569-9ab3-cc625a15101c",
   "metadata": {},
   "source": [
    "We initialize an `AdamW` optimiser, and then start the main training loop. Which consists of a model forward pass, followed by a backward pass and optimiser step. \n",
    "\n",
    "Note that the model takes a number of arguments:\n",
    "- `labels` (optional) which are the ground truth labels (for calculating the loss function when training),\n",
    "- `input_ids` which are the padded sequences of word-pieces, and\n",
    "- `attention_mask` which is an binary mask indicating which parts of the inputs are actual tokens and which are padding tokens (we do not want the transformer to pay attention to any padding tokens);\n",
    "\n",
    "and returns a [SequenceClassifierOutput](https://huggingface.co/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6efe71-e851-424f-b1dd-aaa4bae07584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(model, dataset_train, dataset_val=None, eval_fn=None, batch_size=8, n_epochs=2, learning_rate=1e-5):\n",
    "    model.train(True)\n",
    "    \n",
    "    # create a pytorch data loaders to make it easy to iterate over batches of training data\n",
    "    # see https://pytorch.org/docs/stable/data.html\n",
    "    dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    n_batches = (len(dataset_train) - 1) // batch_size + 1\n",
    "    \n",
    "    # get the AdamW optimizer\n",
    "    optimiser = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # run the training loop\n",
    "    print(f'{\"Epoch\":>10} {\"Batch\":>20} {\"Loss\":>10}')\n",
    "    for epoch in range(n_epochs):\n",
    "        for (b, batch) in enumerate(tqdm(dataloader)):\n",
    "    \n",
    "            # run the transformer forwards\n",
    "            outputs = model(\n",
    "                labels = batch[\"label\"].to(device),\n",
    "                input_ids = batch[\"input_ids\"].to(device),\n",
    "                attention_mask = batch[\"attention_mask\"].to(device),\n",
    "            )\n",
    "    \n",
    "            # get the classification loss\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # backpropagate then apply the optimiser\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "    \n",
    "            # print the loss\n",
    "            if (b+1) % 5 == 0:\n",
    "                print(f'{epoch+1:>10} {f\"{b+1} / {n_batches}\":>20} {format(loss.cpu().item(), \".3f\"):>10}')\n",
    "\n",
    "        # evaluate the model on validation data\n",
    "        if (dataset_val is not None) and (eval_fn is not None):\n",
    "            print('Evaluating ...')\n",
    "            f1 = eval_fn(model, dataset_val)\n",
    "            print(f'Epoch {epoch+1}: F1 score (validation) = {format(f1, \".3f\")}') \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d99068f-83bc-4b57-928b-71ea51bef2a5",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Implement a `evaluate_model` function which returns the F1 score of the model on the validation data. This is going to be similar to the evaluation functions you have seen in previous labs/assignments and also reasonably similar to the training loop provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e2ed4e-5c36-42ec-8eb5-a365520b0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset_val, batch_size=8):\n",
    "    # TODO: Implement this function which returns the F1 score of the model on the validation data.\n",
    "    model.train(False)\n",
    "    f1 = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    model.train(True)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932797f-aa30-4f7b-9f2c-07bd491bf142",
   "metadata": {},
   "source": [
    "Fine tune the model and evaluate it on the validation set after each training epoch (this may take ~30 minutes if using CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d529e89-e968-4042-911f-932afff288c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = finetune_model(model, small_train_data, small_val_data, eval_fn=evaluate_model, n_epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
